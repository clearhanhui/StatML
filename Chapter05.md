[TOC]
# 5.1决策树
**决策树（decision tree）**是一种分类和回归算法。它是描述对实例进行分类的属性结构。树中的节点分为叶节点和内部节点两种：叶节点表示一个类，内部节点表示一种特征。  
数据集往往有太多特征个数，从众多决策树中选取最优的决策树是一个NP完全问题。所以通常采取启发式的方法，近似求解这一最优化问题，这样得到的决策树是次最优的。通常的做法是递归的选取最优的特征，并基于该特征对数据集分割，分割后的数据递归进行上述过程，直到所有数据被正常分类或触发结束条件。
# 5.2 特征选择
**熵（entropy)**是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i) = p_i, i =1,2,3...,n
$$
则随机变量$X$的熵定义为：
$$
H(X) = -\sum_{i=1}^np_i\log p_i \tag{5.1}
$$
熵越大，随机变量的不确定性就越大。  
**信息增益**$g$，特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即互信息：
$$
H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|} \tag{5.2}
$$
$$
H(D|A)=-\sum_{i=1}^n\frac{|D_i|}{|D|} \\
g(D,A) = H(D) - H(D|A)
$$
其中，$C_k$是集合$D$中第$k$个类的样本集合。  
**信息增益比**是为了解决信息增益偏向于选择取值较多的特征的问题。定义为信息增益$g(D,A)$与训练数据集D关羽特征A的值的熵$H_A(D)$之比：
$$
g_R(D,A) = \frac{g(D,A)}{H_A(D)} \tag{5.5}
$$
其中，$H_A(D)=-\sum\limits_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}$。  
**基尼指数**$Gini$，在分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：
$$
Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2 \tag{5.6}
$$
同理，样本集合D的基尼指数为：
$$
Gini(D) = 1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2 \tag{5.7}
$$
给定条件A下，集合D基尼指数：
$$
% \begin{equation}
Gini(D,A)=\sum_{i=1}^a \frac{|D_i|}{|D|}Gini(D_i) \tag{5.8}
% \end{equation}
$$
# 5.3 决策树生成算法
### 5.3.1 ID3算法
输入：训练数据集$D$，特征集$A$，阈值$\epsilon$；  
输出：决策树$T$。
1. 若$D$种所有的实例属于同一类$C_k$，则$T$为单结点树吗，并将类$C_k$作为该结点的类标记，返回$T$；
2. 若$A=\empty$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$；
3. 否则按照信息增益算法，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；
4. 如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；
5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由结点及其子结点构成树$T$，返回$T$；
6. 对第$i$个子节点，以$D_i$为训练集，以$A-\{Ag\}$为特征集，递归调用步骤1~5得到子树$T_i$，返回$T_i$。
### 5.3.2 C4.5
C4.5在生成的过程中，用信息增益比来选择特征。
# 5.4 决策树剪枝
决策树的生成过程中，往往产生过拟合现象，对训练数据集分类很准确，对位置的测试数据分类不准确。可以应用**剪枝**的技术从树上剪掉子树或者叶节点简化模型。  
设树$T$的叶节点个数为$|T|$，$t$是一个叶节点，一个叶节点由$N_t$个样本点，$k$类的样本有$N_{tk}$个，$H_t(T)$是叶节点t的经验熵，则一个损失函数可以定义为：
$$
C_{\alpha}(T) = \sum^{|T|}_{t=1} N_t H_t(T) + \alpha |H|
$$
$$
H_t(T) = -\sum_k \frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}
$$
**剪枝算法**：
1. 计算每个节点的经验熵；
2. 递归从数的叶节点向上回缩，计算回缩前后的损失函数值，若损失函数减少，则剪枝；
3. 重复步骤2，直至收敛。

# 5.5 CART
这里只介绍回归树。  
设X,Y分别表示输入和输出，并且Y是连续变量，训练数据集：  
$$
D=\{(x_1, y_1),...,(x_N,y_N)\}
$$
若树已经将空间划分为M个单元$\{R_1,...,R_M\}$，回归树模型可以表示为：  
$$
f(x)=\sum\limits^M_{m=1} c_m I(x\in R_m)
$$
损失函数用平方误差表示$\sum\limits_{x_i\in R_m}(y_i - f(x_i))$，则$R_m$上$c_m$最优值$\hat{c}_m$为所有实例输出$y_i$的均值：
$$
\hat{c}_m = mean(y_i|x_i \in R_m)
$$
选择第$x$的第$j$维数据$x^j$和$x^j$的一个取值$s$，可以划分：
$$
R_l = \{x|x^j\leq s\} 和 R_r=\{x|x^j > s\}
$$
优化下面公式：
$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$
最优切分点是均值，可以根据公式$$求得。

**算法**：
输入：训练数据
输出：CART树
1. 遍历j和s，求解$$，选择最优的j和s。
2. 用选定的j和s划分区域，并根据$$求解输出值。
3. 重复步骤1和步骤2，直至收敛。

分类树使用**基尼指数**表示不确定性。

**CART剪枝**
