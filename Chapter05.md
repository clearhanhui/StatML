[TOC]
# 5.1决策树
**决策树（decision tree）**是一种分类和回归算法。它是描述对实例进行分类的属性结构。树中的节点分为叶节点和内部节点两种：叶节点表示一个类，内部节点表示一种特征。  
数据集往往有太多特征个数，从众多决策树中选取最优的决策树是一个NP完全问题。所以通常采取启发式的方法，近似求解这一最优化问题，这样得到的决策树是次最优的。通常的做法是递归的选取最优的特征，并基于该特征对数据集分割，分割后的数据递归进行上述过程，直到所有数据被正常分类或触发结束条件。
# 5.2 特征选择
**熵（entropy）**是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i) = p_i, i =1,2,3...,n
$$
则随机变量$X$的熵定义为：
$$
H(X) = -\sum_{i=1}^np_i\log p_i \tag{5.1}
$$
熵越大，随机变量的不确定性就越大。  
**信息增益**$g(D,A)$，特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即互信息：
$$
H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
$$
$$
H(D|A)=-\sum_{i=1}^n\frac{|D_i|}{|D|}
$$
$$
g(D,A) = H(D) - H(D|A) \tag{5.2}
$$
其中，$C_k$是集合$D$中第$k$个类的样本集合。  
**信息增益比**是为了解决信息增益偏向于选择取值较多的特征的问题。定义为信息增益$g(D,A)$与训练数据集D关羽特征A的值的熵$H_A(D)$之比：
$$
g_R(D,A) = \frac{g(D,A)}{H_A(D)} \tag{5.3}
$$
其中，$H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}$。  
**基尼指数**$Gini(p)$，在分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：
$$
Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2 \tag{5.4}
$$
同理，样本集合D的基尼指数为：
$$
Gini(D) = 1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2 \tag{5.5}
$$
给定条件A下，集合D基尼指数：
$$
% \begin{equation}
Gini(D,A)=\sum_{i=1}^a \frac{|D_i|}{|D|}Gini(D_i) \tag{5.6}
% \end{equation}
$$
# 5.3 决策树生成算法
### 5.3.1 ID3算法
输入：训练数据集$D$，特征集$A$，阈值$\epsilon$；  
输出：决策树$T$。
1. 若$D$种所有的实例属于同一类$C_k$，则$T$为单结点树吗，并将类$C_k$作为该结点的类标记，返回$T$；
2. 若$A=\empty$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$；
3. 否则按照信息增益算法，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；
4. 如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；
5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由结点及其子结点构成树$T$，返回$T$；
6. 对第$i$个子节点，以$D_i$为训练集，以$A-\{Ag\}$为特征集，递归调用步骤1~5得到子树$T_i$，返回$T_i$。
### 5.3.2 C4.5
C4.5在生成的过程中，用信息增益比来选择特征。
# 5.4 决策树剪枝
决策树的生成过程中，往往产生过拟合现象，对训练数据集分类很准确，对位置的测试数据分类不准确。可以应用**剪枝**的技术从树上剪掉子树或者叶节点简化模型。  

# 5.5 CART
