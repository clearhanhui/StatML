[TOC]
# 3.1 K近邻算法
KNN是一种十分容易理解的分类和回归模型。设输入为：
$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
其中，$x_i\in X \subseteq R^n$为实例的特征向量，$y_i\in Y=\{c_1,c_2,...,c_3\}$为实例的类别，$i=1,2,...,N$；根据给定的距离度量算法，在训练集$T$中找出与$x$最邻近的$k$个点，在涵盖这$k$个点的$x$的邻域$N_k(x)$中根据分类决策规则决定$x$的类别$y$：
$$y=arg\ \underset{c_j}{max}\sum_{x\in N_k(x)}I(y_i=c_j),\ i=1,2,...,N,\ j=1,2,...,K$$
其中$I$为指示函数，即当$y_i=c_j$时$I=1$，否则$I=0$。
# 3.2 K近邻模型
## 3.2.1 模型
（略）
## 3.2.2 距离度量
常用的距离有欧氏距离，更具有一般性的$L_p$距离（Minkowski距离)。设特征空间$X$是$n$维实数向量空间$R^n$，$x_i$，$x_j$的$L_p$距离定义为：
$$L_p(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$$
其中$p\geq 1$。
* 当$p=1$时称为曼哈顿距离；
* 当$p=2$时称为欧氏距离；
* 当$p=+\infty$时
$$L_{+\infty}(x_i,x_j)=\underset{l}{max}|x_i^{(l)}-x_j^{(l)}|$$
## 3.2.3 K值的选择b
$K$减小意味着整体模型变得复杂，容易发生过拟合；$K$增大意味着模型变得简单。
$K=1$时称为最临近算法；$K=N$时总是输出训练样本中样本最多的一类。
通常采用交叉验证选择合适的$K$值。
## 3.2.4 分类决策规则
$K$近邻常用的分类决策规则是多数表决法。设损失函数是0-1损失函数，分类函数是
$$f:R^n\rightarrow \{c_1,c_2,...,c_N\}$$
对于给定的实例$x\in X$，其中最邻近的$K$个训练实例点构成集合$N_K(x)$，设类别是$c_j$那么误分类的概率是
$$P(Y\neq f(X))=1-P(Y=f(X))=\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\neq c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i=c_j)$$
要是误分类率最小，就要使$\sum_{x_i\in N_k(x)}I(y_i=c_j)$最大，所以多数表决规则等价于经验风险最小化。
# 3.3 kd树
kd树是一种对k维空间之中的实例点进行存储以进行快速搜索的二叉树。当实例点是随机分布时，kd书的搜索平均计算复杂度是$O(log\ N)$，N是训练实例数。适用于训练实例数远大于空间维度的搜索任务，当维度接近训练实例数时，效率几乎等同于线性扫描。
## 3.3.1 构造kd树
输入：k维空间数据集$T=\{x_1,x_2,...,x_N\}$，其中$x_i=\{x_i^1,x_i^2,...,x_i^k\}^T$，$i=1,2,...,N$;
输出：kd树。
**构造算法**：
1. 开始:
   * 构造根结点，根结点对应于包含$T$的$k$维空间的超矩形区域。
   * 选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
   * 由根结点生成深度为1的左、右子结点:左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应于坐标$x^{(1)}$大于切分点的子区域。将落在切分超平面上的实例点保存在根结点。
2. 重复:
   * 对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(mod\ k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。
   * 由该结点生成深度为$j+1$的左、右子结点:左子结点对应坐标$x^{(l)}$小于切分点的子区域，右子结点对应坐标x()大于切分点的子区域。
   * 将落在切分超平面上的实例点保存在该结点。
3. 直到两个子区域没有实例存在时停止。从而形成kd树的区域划分.
[\\]:[](https://img2020.cnblogs.com/blog/2043903/202005/2043903-20200527071112097-118542284.png)


## 3.3.2 搜索kd树
给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点;然后从该叶结点出发，依次回退到父结点;不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。
输入：kd树；
输出：x的最邻近
**搜索算法**
1. 在kd树中找出包含目标点x的叶结点:从根结点出发,递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。
2. 以此叶结点为“当前最近点”。
3. 递归地向上回退，在每个结点进行以下操作:
   * 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。
   * 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。
   * 如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着,递归地进行最近邻搜索;
   * 如果不相交，向上回退。
4. 当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。

----
kd树这里直接摘的原书内容，文字表述不是很方便，更多信息和具体实现参考[这里](https://www.cnblogs.com/earendil/p/8135074.html)。
