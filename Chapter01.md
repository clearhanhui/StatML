[TOC]
我有一点点机器学习的基础，这一章阅读起来还是比较轻松。
# 1.1 统计学习
## 1.1.1 背景
这里首先要介绍一下**贝叶斯派**和**频率派**。举个栗子：一个病毒检测试剂，如果受检者为阳性，误检率为1%，如果受检者为阴性，误检率为2%；若人群中某个人的检测结果为阳性，则此人携带病毒的概率是多少。概率派观点是，此人极有可能使病毒携带者；从贝叶斯的角度来看，我们还需要知道另外一个参数，即人群中携带病毒的概率（先验概率）。
**极大似然估计**是**频率派**的主要观点，核心思想是对于待估参数$\theta$，寻找$\hat{\theta}$使得$X=x$发生的概率最大
$$L(\theta|x)=f(x|\theta)=f(x_1,x_2...x_n|\theta)=\prod_{i=1}^n f(x_i|\theta)$$
$$\hat{\theta}=argmax_{\theta}\ L(\theta|x)$$
**最大后验估计**是**贝叶斯派**的主要观点，它简化了**贝叶斯估计**
$$\hat{\theta}=argmax_{\theta}\ \pi (\theta|x) = argmax_{\theta}\ \frac {f(x|\theta) \pi(\theta)} {m(x)} = argmax_{\theta}\ f(x|\theta) \pi(\theta) = argmax_{\theta}\{log[f(x|\theta)] + log[\pi(\theta)]\}$$
>贝叶斯估计与极大似然估计在思想上有很大的不同,代表着统计学中频率学派和贝叶斯学派对统计的不同认识。其实，可以简单地把两者联系起来，假设先验分布是均匀分布，取后验概率最大，就能从贝叶斯估计得到极大似然估计。
## 1.1.2 统计学习分类
* 基本分类：监督式学习，无监督式学习，强化学习，半监督学习，主动学习。
* 模型分类：概率模型和非概率模型、线性模型和非线性模型、参数模型和非参数模型
   * 概率模型：决策树、朴素贝叶斯、隐马尔科夫模型；非概率模型：感知机、SVM、KNN、K-means
   * 参数模型假设模型的参数固定，或模型的参数是有限的；非常参数模型是模型参数的不固定，随着训练改变。参数模型：K-means、感知机、朴素贝叶斯；非参数模型：SVM、KNN、决策树
* 算法分类：在线学习、批量学习
* 技巧分类：贝叶斯学习、核技巧

## 1.1.3 统计学习方法三要素
统计学习方法 = 模型 + 策略 + 算法
* 模型：在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。（不是很懂）
* 策略：损失函数和风险函数、经验风险最小和结构风险最小
  * 损失函数：0-1损失函数，平方损失函数，绝对值损失函数，对数损失函数
  * 风险：期望风险是模型关于联合分布的期望损失；经验风险是模型关于训练样本集的平均损失
  * 经验风险最小化：类似极大似然估计的思想，但是容易过拟合
  * 结构风险最小化：带有正则化项的模型，可以参照最大后验概率估计。下式$J(f)$代表模型的复杂度
  $$R_{srm} (f) = \frac {1} {N} \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)$$
* 算法：算法是模型的具体计算方法。最优化问题无解析解时，需要通过数值计算方法求解


* **模型的选择和评估、过拟合和模型选择、正则化和交叉验证**，这几个章节没什么重要内容，主要理解过拟合产生的机制、表现和应对办法（增加惩罚项）即可*


# 1.2 泛化能力
学习方法的泛化能力指的是由该方法得到的模型处理未知数据的能力。
* 泛化误差：反应学习方法的泛化能力，其值越小，该方法就更有效
$$ R_{exp}(\hat{f}) = E_p[L(Y,\hat{f}(X))] = \int_{X×Y} L(y,\hat{f}(x)) P(x,y)dxdy $$
其中$\hat{f}$代表学习到的模型。泛化误差就是模型的期望风险。
* 泛化误差上界：
>学习方法的泛化能力分析往往是通过研究泛化误差的概率.上界进行的，简称为泛化误差上界(generalization error bound)。具体来说，就是通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣。泛化误差上界通常具有以下性质:它是样本容量的函数，当样本容量增加时，泛化上界趋于0;它是假设空间容量(capacity) 的函数,假设空间容量越大，模型就越难学,泛化误差上界就越大。

**定理**：对于二分类问题，当假设空间是有限个函数的集合$F={f_1,f_2,...,f_d}$时，对于一个任意函数$f \in F$,至少以概率$1-\delta$(其中$0<\delta<1$)，以下不等式成立：
$$R(f)<=\hat{R}(f)+\epsilon(d,N,\delta)$$
其中，$R$是期望风险,$\hat{R}$是经验风险,$N$是样本容量
$$\epsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(log\ d+log\ \frac{1}{\delta})}$$
# 1.3 生成模型和判别模型
生成模型：表示了给定的输入$X$产生输出$Y$的生成关系。朴素贝叶斯、隐马尔科夫模型等。
$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$
判别模型：表示给定输入X产生输出Y的值。KNN，感知机，逻辑回归等。
# 1.4 监督学习应用
## 1.4.1 分类问题
当输出变量Y的取值为有限个离散值时，预测问题变为分类问题。
* 二分类的评价指标
  TP将正类预测为正类数；
  FN将正类预测为负类数；
  FP将负类预测为正类数；
  TN将负类预测为负类数。
  * **精确率**：
  $$P=\frac{TP}{TP+FP}$$
  * **召回率**:
  $$P=\frac{TP}{TP+FN}$$
  * **$F_1$值**：精准率和召回率的调和均值
  $$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$
## 1.4.2 标注问题
标注问题也是一种监督学习的问题，可以认为是分类问题的推广，其目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。隐马尔科夫模型、条件随机场等。
## 1.4.3 回归问题
连续值的预测，典型案例：波士顿房价

-------------------------------------------------

简单写一下感受：
* 这篇文章的内容基本都是《统计学习方法》的上介绍的，我当作学习笔记用，所以详略安排的很不合理；
* 对于贝叶斯派和概率派的观点理解并不全面，不敢深入介绍，更多内容和MAP与MLE的关系可以参考 [这里](https://zhuanlan.zhihu.com/p/40024110)；
* 公式编写还不够熟练，且没有边写边看的功能，很耽误功夫，也怪自己做不到笔下是语法，心中格式化，以后准备先用vs code写，再复制到博客园；
